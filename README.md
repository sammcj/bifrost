# Bifrost

[![Go Report Card](https://goreportcard.com/badge/github.com/maximhq/bifrost/core)](https://goreportcard.com/report/github.com/maximhq/bifrost/core)

**The fastest way to build AI applications that never go down.**

Bifrost is a high-performance AI gateway that connects you to 10+ providers (OpenAI, Anthropic, Bedrock, and more) through a single API. Get automatic failover, load balancing, and zero-downtime deployments in under 30 seconds.

![Bifrost](./docs/media/cover.png)

ğŸš€ **Just launched:** Native MCP (Model Context Protocol) support for seamless tool integration  
âš¡ **Performance:** Adds only 11Âµs latency while handling 5,000+ RPS  
ğŸ›¡ï¸ **Reliability:** 100% uptime with automatic provider failover

## âš¡ Quickstart (30 seconds)

**Go from zero to production-ready AI gateway in under a minute.** Here's how:

**What You Need**

- Any AI provider API key (OpenAI, Anthropic, Bedrock, etc.)
- Node.js 18+ installed (or use Docker instead via [Docker installation](./docs/quickstart/http-transport.md))
- 20 seconds of your time â°

### Using Bifrost HTTP Transport

ğŸ“– For detailed setup guides with multiple providers, advanced configuration, and language examples, see [Quick Start Documentation](./docs/quickstart/http-transport.md)

**Step 1:** Start Bifrost

```bash
# ğŸ”§ Run Bifrost binary
npx @maximhq/bifrost
```

**Step 2:** Open the built-in web interface and configure bifrost

```bash
# ğŸ–¥ï¸ Open the web interface in your browser
open http://localhost:8080

# Or simply open http://localhost:8080 manually in your browser
```

**Step 3:** Test it works

```bash
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "openai/gpt-4o-mini",
    "messages": [
      {"role": "user", "content": "Hello from Bifrost! ğŸŒˆ"}
    ]
  }'
```

**ğŸ‰ Boom! You're done!**

Your AI gateway is now running with a beautiful web interface. You can:

- **ğŸ–¥ï¸ Configure everything visually** - No more JSON files!
- **ğŸ“Š Monitor requests in real-time** - See logs, analytics, and metrics
- **ğŸ”„ Add providers and MCP clients on-the-fly** - Scale and failover without restarts
- **ğŸš€ Drop into existing code** - Zero changes to your OpenAI/Anthropic apps

> **Want more?** See our [Complete Setup Guide](./docs/quickstart/http-transport.md) for multi-provider configuration, failover strategies, and production deployment.

## ğŸ“‘ Table of Contents

- [Bifrost](#bifrost)
  - [âš¡ Quickstart (30 seconds)](#-quickstart-30-seconds)
    - [Using Bifrost HTTP Transport](#using-bifrost-http-transport)
  - [ğŸ“‘ Table of Contents](#-table-of-contents)
  - [âœ¨ Features](#-features)
  - [ğŸ—ï¸ Repository Structure](#ï¸-repository-structure)
  - [ğŸš€ Getting Started](#-getting-started)
    - [1. As a Go Package (Core Integration)](#1-as-a-go-package-core-integration)
    - [2. As an HTTP API (Transport Layer)](#2-as-an-http-api-transport-layer)
    - [3. As a Drop-in Replacement (Zero Code Changes)](#3-as-a-drop-in-replacement-zero-code-changes)
  - [ğŸ“Š Performance](#-performance)
    - [ğŸ”‘ Key Performance Highlights](#-key-performance-highlights)
  - [ğŸ“š Documentation](#-documentation)
  - [ğŸ’¬ Need Help?](#-need-help)
  - [ğŸ¤ Contributing](#-contributing)
  - [ğŸ“„ License](#-license)

---

## âœ¨ Features

- **ğŸ–¥ï¸ Built-in Web UI**: Visual configuration, real-time monitoring, and analytics dashboard - no config files needed
- **ğŸš€ Zero-Config Startup & Easy Integration**: Start immediately with dynamic provider configuration, or integrate existing SDKs by simply updating the `base_url` - one line of code to get running
- **ğŸ”„ Multi-Provider Support**: Integrate with OpenAI, Anthropic, Amazon Bedrock, Mistral, Ollama, and more through a single API
- **ğŸ›¡ï¸ Fallback Mechanisms**: Automatically retry failed requests with alternative models or providers
- **ğŸ”‘ Dynamic Key Management**: Rotate and manage API keys efficiently with weighted distribution
- **âš¡ Connection Pooling**: Optimize network resources for better performance
- **ğŸ¯ Concurrency Control**: Manage rate limits and parallel requests effectively
- **ğŸ”Œ Flexible Transports**: Multiple transports for easy integration into your infra
- **ğŸ—ï¸ Plugin First Architecture**: No callback hell, simple addition/creation of custom plugins
- **ğŸ› ï¸ MCP Integration**: Built-in Model Context Protocol (MCP) support for external tool integration and execution
- **âš™ï¸ Custom Configuration**: Offers granular control over pool sizes, network retry settings, fallback providers, and network proxy configurations
- **ğŸ“Š Built-in Observability**: Native Prometheus metrics out of the box, no wrappers, no sidecars, just drop it in and scrape
- **ğŸ”§ SDK Support**: Bifrost is available as a Go package, so you can use it directly in your own applications

---

## ğŸ—ï¸ Repository Structure

Bifrost is built with a modular architecture:

```text
bifrost/
â”œâ”€â”€ ci/                   # CI/CD pipeline scripts and npx configuration
â”‚
â”œâ”€â”€ core/                 # Core functionality and shared components
â”‚   â”œâ”€â”€ providers/        # Provider-specific implementations
â”‚   â”œâ”€â”€ schemas/          # Interfaces and structs used in bifrost
â”‚   â”œâ”€â”€ bifrost.go        # Main Bifrost implementation
â”‚
â”œâ”€â”€ docs/                 # Documentations for Bifrost's configurations and contribution guides
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ tests/                # All test setups related to /core and /transports
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ transports/           # Interface layers (HTTP, gRPC, etc.)
â”‚   â”œâ”€â”€ bifrost-http/     # HTTP transport implementation
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ ui/                  # UI files for the web interface of the HTTP transport
â”‚   â””â”€â”€ ...
â”‚
â””â”€â”€ plugins/              # Plugin Implementations
    â”œâ”€â”€ maxim/
    â””â”€â”€ ...
```

The system uses a provider-agnostic approach with well-defined interfaces to easily extend to new AI providers. All interfaces are defined in `core/schemas/` and can be used as a reference for contributions.

---

## ğŸš€ Getting Started

There are three ways to use Bifrost - choose the one that fits your needs:

### 1. As a Go Package (Core Integration)

For direct integration into your Go applications. Provides maximum performance and control.

> **ğŸ“– [2-Minute Go Package Setup](./docs/quickstart/go-package.md)**

Quick example:

```bash
go get github.com/maximhq/bifrost/core
```

### 2. As an HTTP API (Transport Layer)

For language-agnostic integration and microservices architecture.

> **ğŸ“– [30-Second HTTP Transport Setup](./docs/quickstart/http-transport.md)**

Quick example:

```bash
npx @maximhq/bifrost
```

### 3. As a Drop-in Replacement (Zero Code Changes)

Replace existing OpenAI/Anthropic APIs without changing your application code.

> **ğŸ“– [1-Minute Drop-in Integration](./docs/usage/http-transport/integrations/README.md)**

Quick example:

```diff
- base_url = "https://api.openai.com"
+ base_url = "http://localhost:8080/openai"
```

---

## ğŸ“Š Performance

**Bifrost adds virtually zero overhead to your AI requests.** In our sustained 5,000 RPS benchmark (see full methodology in [docs/benchmarks.md](./docs/benchmarks.md)), the gateway added only **11 Âµs** of overhead per request â€“ that's **less than 0.001%** of a typical GPT-4o response time.

**Translation:** Your users won't notice Bifrost is there, but you'll sleep better knowing your AI never goes down.

| Metric                                | t3.medium | t3.xlarge   | Î”                  |
| ------------------------------------- | --------- | ----------- | ------------------ |
| Added latency (Bifrost overhead)      | 59 Âµs     | **11 Âµs**   | **-81 %**          |
| Success rate @ 5 k RPS                | 100 %     | 100 %       | No failed requests |
| Avg. queue wait time                  | 47 Âµs     | **1.67 Âµs** | **-96 %**          |
| Avg. request latency (incl. provider) | 2.12 s    | **1.61 s**  | **-24 %**          |

### ğŸ”‘ Key Performance Highlights

- **Perfect Success Rate** â€“ 100 % request success rate on both instance types even at 5 k RPS.
- **Tiny Total Overhead** â€“ < 15 Âµs additional latency per request on average.
- **Efficient Queue Management** â€“ just **1.67 Âµs** average wait time on the t3.xlarge test.
- **Fast Key Selection** â€“ ~**10 ns** to pick the right weighted API key.

Bifrost is deliberately configurable so you can dial the **speed â†” memory** trade-off:

| Config Knob                   | Effect                                                           |
| ----------------------------- | ---------------------------------------------------------------- |
| `initial_pool_size`           | How many objects are pre-allocated. Higher = faster, more memory |
| `buffer_size` & `concurrency` | Queue depth and max parallel workers (can be set per provider)   |
| Retry / Timeout               | Tune aggressiveness for each provider to meet your SLOs          |

Choose higher settings (like the t3.xlarge profile above) for raw speed, or lower ones (t3.medium) for reduced memory footprint â€“ or find the sweet spot for your workload.

> **Need more numbers?** Dive into the [full benchmark report](./docs/benchmarks.md) for breakdowns of every internal stage (JSON marshalling, HTTP call, parsing, etc.), hardware sizing guides and tuning tips.

---

## ğŸ“š Documentation

**Everything you need to master Bifrost, from 30-second setup to production-scale deployments.**

<details>
<summary><strong>ğŸš€ I want to get started (2 minutes)</strong></summary>

- **[ğŸ“– Documentation Hub](./docs/README.md)** - Your complete roadmap to Bifrost
- **[ğŸ”§ Go Package Setup](./docs/quickstart/go-package.md)** - Direct integration into your Go app
- **[ğŸŒ HTTP API Setup](./docs/quickstart/http-transport.md)** - Language-agnostic service deployment
- **[ğŸ”„ Drop-in Replacement](./docs/usage/http-transport/integrations/README.md)** - Replace OpenAI/Anthropic with zero code changes

</details>

<details>
<summary><strong>ğŸ¯ I want to understand what Bifrost can do</strong></summary>

- **[ğŸ”— Multi-Provider Support](./docs/usage/providers.md)** - Connect to 10+ AI providers with one API
- **[ğŸ›¡ï¸ Fallback & Reliability](./docs/usage/providers.md#fallback-mechanisms)** - Never lose a request with automatic failover
- **[ğŸ› ï¸ MCP Tool Integration](./docs/usage/http-transport/configuration/mcp.md)** - Give your AI external capabilities
- **[ğŸ”Œ Plugin Ecosystem](./docs/usage/http-transport/configuration/plugins.md)** - Extend Bifrost with custom middleware
- **[ğŸ”‘ Key Management](./docs/usage/key-management.md)** - Rotate API keys without downtime
- **[ğŸ“¡ Networking](./docs/usage/networking.md)** - Proxies, timeouts, and connection tuning

</details>

<details>
<summary><strong>âš™ï¸ I want to deploy this to production</strong></summary>

- **[ğŸ—ï¸ System Architecture](./docs/architecture/README.md)** - Understand how Bifrost works internally
- **[ğŸ“Š Performance Tuning](./docs/benchmarks.md)** - Squeeze out every microsecond
- **[ğŸš€ Production Deployment](./docs/usage/http-transport/README.md)** - Scale to millions of requests
- **[ğŸ”§ Complete API Reference](./docs/usage/README.md)** - Every endpoint, parameter, and response
- **[ğŸ› Error Handling](./docs/usage/errors.md)** - Troubleshoot like a pro

</details>

<details>
<summary><strong>ğŸ“± I'm migrating from another tool</strong></summary>

- **[ğŸ”„ Migration Guides](./docs/usage/http-transport/integrations/migration-guide.md)** - Step-by-step migration from OpenAI, Anthropic, LiteLLM
- **[ğŸ“ Real-World Examples](./docs/examples/)** - Production-ready code samples
- **[â“ Common Questions](./docs/usage/errors.md)** - Solutions to frequent issues

</details>

---

## ğŸ’¬ Need Help?

**ğŸ”— [Join our Discord](https://getmax.im/bifrost-discord)** for:

- â“ Quick setup assistance and troubleshooting
- ğŸ’¡ Best practices and configuration tips
- ğŸ¤ Community discussions and support
- ğŸš€ Real-time help with integrations

---

## ğŸ¤ Contributing

See our **[Contributing Guide](./docs/contributing/README.md)** for detailed information on how to contribute to Bifrost. We welcome contributions of all kindsâ€”whether it's bug fixes, features, documentation improvements, or new ideas. Feel free to open an issue, and once it's assigned, submit a Pull Request.

---

## ğŸ“„ License

This project is licensed under the Apache 2.0 License - see the [LICENSE](LICENSE) file for details.

Built with â¤ï¸ by [Maxim](https://github.com/maximhq)
