# ğŸŒ HTTP API Endpoints

Complete reference for Bifrost HTTP transport API endpoints and usage patterns.

> **ğŸ’¡ Quick Start:** See the [30-second setup](../../quickstart/http-transport.md) for basic API usage.

---

## ğŸ“‹ Endpoint Overview

Bifrost HTTP transport provides:

- **Unified API endpoints** for all providers
- **Drop-in compatible endpoints** for existing SDKs
- **MCP tool execution** endpoint
- **Prometheus metrics** endpoint

Base URL: `http://localhost:8080` (configurable)

---

## ğŸ”„ Unified API Endpoints

> All endpoints and request/response formats are **OpenAI compatible**.

### **POST /v1/chat/completions**

Chat conversation endpoint supporting all providers.

**Request Body:**

```json
{
  "model": "openai/gpt-4o-mini",
  "messages": [
    {
      "role": "user",
      "content": "Hello, how are you?"
    }
  ],
  "temperature": 0.7,
  "max_tokens": 1000,
  "fallbacks": ["anthropic/claude-3-sonnet-20240229"]
}
```

**Response:**

```json
{
  "id": "chatcmpl-123",
  "object": "chat.completion",
  "created": 1677652288,
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "Hello! I'm doing well, thank you for asking."
      },
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 9,
    "completion_tokens": 12,
    "total_tokens": 21
  }
}
```

**cURL Example:**

```bash
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "openai/gpt-4o-mini",
    "messages": [
      {"role": "user", "content": "What is the capital of France?"}
    ]
  }'
```

### **Streaming Responses**

To receive a stream of partial responses, set `"stream": true` in your request. The response will be a `text/event-stream` of Server-Sent Events (SSE).

**Request with Streaming:**

```json
{
  "model": "openai/gpt-4o-mini",
  "messages": [{"role": "user", "content": "Write a short story."}],
  "stream": true
}
```

**SSE Event Stream:**

Each event in the stream is a JSON object prefixed with `data: `. The stream is terminated by a `[DONE]` message.

```
data: {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1694268190,"model":"gpt-4o-mini","choices":[{"index":0,"delta":{"role":"assistant"},"finish_reason":null}]}

data: {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1694268190,"model":"gpt-4o-mini","choices":[{"index":0,"delta":{"content":"Once"},"finish_reason":null}]}

data: {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1694268190,"model":"gpt-4o-mini","choices":[{"index":0,"delta":{"content":" upon"},"finish_reason":null}]}

data: {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1694268190,"model":"gpt-4o-mini","choices":[{"index":0,"delta":{"content":" a"},"finish_reason":null}]}

data: {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1694268190,"model":"gpt-4o-mini","choices":[{"index":0,"delta":{"content":" time"},"finish_reason":null}]}

data: {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1694268190,"model":"gpt-4o-mini","choices":[{"index":0,"delta":{},"finish_reason":"stop"}]}

data: [DONE]
```

### **POST /v1/text/completions**

Text completion endpoint for simple text generation.

**Request Body:**

```json
{
  "model": "openai/gpt-4o-mini",
  "text": "The future of AI is",
  "temperature": 0.8,
  "max_tokens": 150
}
```

**Response:**

```json
{
  "id": "cmpl-123",
  "object": "text_completion",
  "created": 1677652288,
  "choices": [
    {
      "text": "incredibly promising, with advances in machine learning...",
      "index": 0,
      "finish_reason": "length"
    }
  ],
  "usage": {
    "prompt_tokens": 5,
    "completion_tokens": 150,
    "total_tokens": 155
  }
}
```

### **POST /v1/audio/speech**

ğŸ”Š **Speech synthesis endpoint for converting text to audio.**

**Request Body:**

```json
{
  "model": "openai/tts-1",
  "input": "Hello, this is a test of the speech synthesis feature.",
  "voice": "alloy",
  "response_format": "mp3",
  "instructions": "Speak slowly and clearly"
}
```

**Supported Parameters:**

- `model` (required): Model in "provider/model" format (e.g., "openai/tts-1")
- `input` (required): Text to convert to speech (max 4096 characters)
- `voice` (required): Voice to use - supports:
  - Simple voice: `"alloy"`, `"echo"`, `"fable"`, `"onyx"`, `"nova"`, `"shimmer"`
  - Multi-voice config: `[{"speaker": "narrator", "voice": "alloy"}]`
- `response_format` (optional): Audio format - `"mp3"` (default), `"opus"`, `"aac"`, `"flac"`, `"wav"`, `"pcm"`
- `instructions` (optional): Additional instructions for the voice synthesis
- `stream_format` (optional): Set to `"sse"` for streaming responses

**Response:**

Returns binary audio data with appropriate Content-Type headers:

```
Content-Type: audio/mpeg
Content-Disposition: attachment; filename=speech.mp3
Content-Length: 24576

[Binary audio data]
```

**cURL Example:**

```bash
curl -X POST http://localhost:8080/v1/audio/speech \
  -H "Content-Type: application/json" \
  -d '{
    "model": "openai/tts-1",
    "input": "Hello world! This is a test of text to speech.",
    "voice": "alloy",
    "response_format": "mp3"
  }' \
  --output speech.mp3
```

**Streaming Speech Synthesis:**

```json
{
  "model": "openai/tts-1",
  "input": "This is a longer text that will be streamed as audio chunks.",
  "voice": "nova",
  "response_format": "mp3",
  "stream_format": "sse"
}
```

### **POST /v1/audio/transcriptions**

ğŸ¤ **Audio transcription endpoint for converting speech to text.**

> **Note:** This endpoint expects `multipart/form-data` content type due to file upload requirements.

**Form Data Parameters:**

- `model` (required): Model in "provider/model" format (e.g., "openai/whisper-1")
- `file` (required): Audio file to transcribe (mp3, mp4, mpeg, mpga, m4a, wav, webm)
- `language` (optional): Language code (e.g., "en", "es", "fr") - auto-detected if not specified
- `prompt` (optional): Text prompt to guide the transcription style
- `response_format` (optional): Format of response - `"json"` (default), `"text"`, `"srt"`, `"verbose_json"`, `"vtt"`
- `temperature` (optional): Sampling temperature (0-1) for transcription randomness
- `stream` (optional): Set to `"true"` for streaming transcription

**Response:**

```json
{
  "object": "audio.transcription",
  "text": "Hello, this is a test of the audio transcription feature.",
  "language": "english",
  "duration": 3.45,
  "segments": [
    {
      "id": 0,
      "seek": 0,
      "start": 0.0,
      "end": 3.45,
      "text": "Hello, this is a test of the audio transcription feature.",
      "tokens": [15496, 11, 341, 307, 257, 1500, 295, 264, 6278, 35288, 4122, 13],
      "temperature": 0.0,
      "avg_logprob": -0.23,
      "compression_ratio": 1.2,
      "no_speech_prob": 0.01
    }
  ],
  "usage": {
    "total_duration": 3.45
  }
}
```

**cURL Example:**

```bash
curl -X POST http://localhost:8080/v1/audio/transcriptions \
  -F "model=openai/whisper-1" \
  -F "file=@audio.mp3" \
  -F "language=en" \
  -F "response_format=json"
```

**Streaming Transcription:**

```bash
curl -X POST http://localhost:8080/v1/audio/transcriptions \
  -F "model=openai/whisper-1" \
  -F "file=@audio.mp3" \
  -F "stream=true" \
  -F "response_format=json"
```

**Streaming Response:**

```
data: {"object":"audio.transcription.chunk","text":"Hello","type":"transcript.text.delta"}

data: {"object":"audio.transcription.chunk","text":", this is","type":"transcript.text.delta"}

data: {"object":"audio.transcription.chunk","text":" a test","type":"transcript.text.delta"}

data: {"object":"audio.transcription.chunk","text":"Hello, this is a test of the audio transcription feature.","type":"transcript.text.done"}

data: [DONE]
```

### **POST /v1/mcp/tool/execute**

Direct MCP tool execution endpoint.

**Request Body:**

```json
{
  "id": "call_123",
  "type": "function",
  "function": {
    "name": "read_file",
    "arguments": "{\"path\": \"config.json\"}"
  }
}
```

**Response:**

```json
{
  "role": "tool",
  "content": {
    "content_str": "{\n  \"providers\": {\n    \"openai\": {...}\n  }\n}"
  },
  "tool_call_id": "call_123"
}
```

---

## ğŸ”— Drop-in Compatible Endpoints

### **OpenAI Compatible**

**POST /openai/v1/chat/completions**

Drop-in replacement for OpenAI API:

```bash
curl -X POST http://localhost:8080/openai/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-4o-mini",
    "messages": [
      {"role": "user", "content": "Hello!"}
    ]
  }'
```

### **Anthropic Compatible**

**POST /anthropic/v1/messages**

Drop-in replacement for Anthropic API:

```bash
curl -X POST http://localhost:8080/anthropic/v1/messages \
  -H "Content-Type: application/json" \
  -d '{
    "model": "claude-3-sonnet-20240229",
    "max_tokens": 1000,
    "messages": [
      {"role": "user", "content": "Hello!"}
    ]
  }'
```

### **Google GenAI Compatible**

**POST /genai/v1beta/models/{model}:generateContent**

Drop-in replacement for Google GenAI API:

```bash
curl -X POST http://localhost:8080/genai/v1beta/models/gemini-pro:generateContent \
  -H "Content-Type: application/json" \
  -d '{
    "contents": [{
      "parts": [{"text": "Hello!"}]
    }]
  }'
```

---

## ğŸ“Š Monitoring Endpoints

### **GET /metrics**

Prometheus metrics endpoint:

```bash
curl http://localhost:8080/metrics
```

**Sample Metrics:**

```prometheus
# HELP bifrost_requests_total Total number of requests
# TYPE bifrost_requests_total counter
bifrost_requests_total{provider="openai",model="gpt-4o-mini",status="success"} 1247

# HELP bifrost_request_duration_seconds Request duration in seconds
# TYPE bifrost_request_duration_seconds histogram
bifrost_request_duration_seconds_bucket{provider="openai",le="0.5"} 823
bifrost_request_duration_seconds_bucket{provider="openai",le="1.0"} 1156

# HELP bifrost_provider_errors_total Provider error count
# TYPE bifrost_provider_errors_total counter
bifrost_provider_errors_total{provider="openai",error_type="rate_limit"} 23
```

---

## ğŸ§¾ Redis Cache Management

Bifrost provides built-in Redis caching capabilities to improve performance and reduce API costs.

### **Cache Control Headers**

Add these headers to your requests to control caching behavior:

| Header | Type | Description | Example |
|--------|------|-------------|---------|
| `x-bf-cache-key` | string | Unique cache identifier for the request | `"user-123-session"` |
| `x-bf-cache-ttl` | string | Time-to-live duration for cache entry. Supports Go duration format or plain numbers (treated as seconds) | `"30s"`, `"5m"`, `"1h"`, `"300"` |

**TTL Format Options:**

The `x-bf-cache-ttl` header accepts two formats:

1. **Go Duration Format**: `"30s"`, `"5m"`, `"1h"`, `"2h30m"`
2. **Plain Numbers**: `"30"`, `"300"`, `"3600"` (interpreted as seconds)

**cURL Examples with Caching:**

```bash
# Using Go duration format
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "x-bf-cache-key: user-session-abc123" \
  -H "x-bf-cache-ttl: 10m" \
  -d '{
    "model": "openai/gpt-4o-mini",
    "messages": [
      {"role": "user", "content": "What is the capital of France?"}
    ]
  }'

# Using plain number format (300 seconds = 5 minutes)
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "x-bf-cache-key: user-session-xyz789" \
  -H "x-bf-cache-ttl: 300" \
  -d '{
    "model": "openai/gpt-4o-mini",
    "messages": [
      {"role": "user", "content": "What is the capital of Spain?"}
    ]
  }'
```

**Cache Behavior:**

- **First request**: Goes to the AI provider, response is cached
- **Subsequent identical requests**: Served instantly from Redis cache
- **Cache hits**: Include `bifrost_cached: true` in response metadata
- **Streaming responses**: Cached and reconstructed chunk by chunk

### **DELETE /api/cache/{key}**

Delete a specific cache entry from Redis.

**URL Parameters:**

- `key` (required): The cache key to delete

**cURL Example:**

```bash
curl -X DELETE http://localhost:8080/api/cache/user-session-abc123
```

**Response:**

```json
{
  "status": "success",
  "message": "Redis cache deleted successfully"
}
```

**Notes:**

- For streaming responses, this deletes all chunks associated with the cache key
- Use the `bifrost_cache_key` from cached response metadata to get the exact key
- Returns success even if the key doesn't exist

---

## ğŸ”§ Request Parameters

### **Common Parameters**

| Parameter   | Type   | Description             | Example                                  |
| ----------- | ------ | ----------------------- | ---------------------------------------- |
| `model`     | string | Provider and model name | `"openai/gpt-4o-mini"`                   |
| `fallbacks` | array  | Fallback model names    | `["anthropic/claude-3-sonnet-20240229"]` |

### **Model Parameters**

| Parameter           | Type    | Default          | Description                  |
| ------------------- | ------- | ---------------- | ---------------------------- |
| `temperature`       | float   | 1.0              | Randomness (0.0-2.0)         |
| `max_tokens`        | integer | Provider default | Maximum tokens to generate   |
| `top_p`             | float   | 1.0              | Nucleus sampling (0.0-1.0)   |
| `frequency_penalty` | float   | 0.0              | Frequency penalty (-2.0-2.0) |
| `presence_penalty`  | float   | 0.0              | Presence penalty (-2.0-2.0)  |
| `stop`              | array   | null             | Stop sequences               |

### **Chat Message Format**

```json
{
  "role": "user|assistant|system|tool",
  "content": "text content",
  "tool_calls": [...],
  "tool_call_id": "call_123"
}
```

**Multimodal Content:**

```json
{
  "role": "user",
  "content": [
    {
      "type": "text",
      "text": "What's in this image?"
    },
    {
      "type": "image_url",
      "image_url": {
        "url": "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAA..."
      }
    }
  ]
}
```

---

## ğŸ› ï¸ Tool Calling

### **Automatic Tool Integration**

MCP tools are automatically available in chat completions:

```bash
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "openai/gpt-4o-mini",
    "messages": [
      {"role": "user", "content": "List files in the current directory"}
    ]
  }'
```

**Response with Tool Calls:**

```json
{
  "choices": [
    {
      "message": {
        "role": "assistant",
        "content": null,
        "tool_calls": [
          {
            "id": "call_123",
            "type": "function",
            "function": {
              "name": "list_directory",
              "arguments": "{\"path\": \".\"}"
            }
          }
        ]
      }
    }
  ]
}
```

### **Multi-turn Tool Conversations**

```bash
# Initial request
curl -X POST http://localhost:8080/v1/chat/completions \
  -d '{
    "model": "openai/gpt-4o-mini",
    "messages": [
      {"role": "user", "content": "Read the README.md file"},
      {
        "role": "assistant",
        "tool_calls": [{
          "id": "call_123",
          "type": "function",
          "function": {"name": "read_file", "arguments": "{\"path\": \"README.md\"}"}
        }]
      },
      {
        "role": "tool",
        "content": {"content_str": "# Bifrost\n\nBifrost is..."},
        "tool_call_id": "call_123"
      },
      {"role": "user", "content": "Summarize the main features"}
    ]
  }'
```

---

## ğŸ”„ Error Handling

### **Error Response Format**

```json
{
  "error": {
    "message": "Invalid provider: nonexistent",
    "type": "invalid_request_error",
    "code": "invalid_provider"
  },
  "status_code": 400
}
```

### **Common Error Codes**

| Status | Code                    | Description          |
| ------ | ----------------------- | -------------------- |
| 400    | `invalid_request_error` | Bad request format   |
| 401    | `authentication_error`  | Invalid API key      |
| 403    | `permission_error`      | Access denied        |
| 429    | `rate_limit_error`      | Rate limit exceeded  |
| 500    | `internal_error`        | Server error         |
| 503    | `service_unavailable`   | Provider unavailable |

### **Error Response Examples**

**Missing Provider:**

```json
{
  "error": {
    "message": "Provider is required",
    "type": "invalid_request_error",
    "code": "missing_provider"
  },
  "status_code": 400
}
```

**Rate Limit:**

```json
{
  "error": {
    "message": "Rate limit exceeded for provider openai",
    "type": "rate_limit_error",
    "code": "rate_limit_exceeded"
  },
  "status_code": 429
}
```

---

## ğŸŒ Language SDK Examples

### **Python (OpenAI SDK)**

```python
import openai

client = openai.OpenAI(
    base_url="http://localhost:8080/openai",
    api_key="your-openai-key"
)

response = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=[{"role": "user", "content": "Hello!"}]
)
```

### **JavaScript (OpenAI SDK)**

```javascript
import OpenAI from "openai";

const openai = new OpenAI({
  baseURL: "http://localhost:8080/openai",
  apiKey: process.env.OPENAI_API_KEY,
});

const response = await openai.chat.completions.create({
  model: "gpt-4o-mini",
  messages: [{ role: "user", content: "Hello!" }],
});
```

### **Go (Direct HTTP)**

```go
import (
    "bytes"
    "encoding/json"
    "net/http"
)

type ChatRequest struct {
    Provider string `json:"provider"`
    Model    string `json:"model"`
    Messages []Message `json:"messages"`
}

func makeRequest() {
    req := ChatRequest{
        Provider: "openai",
        Model:    "gpt-4o-mini",
        Messages: []Message{
            {Role: "user", Content: "Hello!"},
        },
    }

    body, _ := json.Marshal(req)
    resp, err := http.Post(
        "http://localhost:8080/v1/chat/completions",
        "application/json",
        bytes.NewBuffer(body),
    )
}
```

---

## ğŸ“š Related Documentation

- **[ğŸŒ HTTP Transport Overview](./README.md)** - Main HTTP transport guide
- **[ğŸ”§ Configuration](./configuration/)** - Provider and MCP setup
- **[ğŸ”— Integrations](./integrations/)** - Drop-in API replacements
- **[ğŸ“ OpenAPI Specification](./openapi.json)** - Complete API schema

> **ğŸ›ï¸ Architecture:** For endpoint implementation details and performance, see [Architecture Documentation](../../architecture/README.md).
