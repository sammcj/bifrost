---
title: "Unified Interface"
description: "Every AI provider returns the same OpenAI-compatible response format, making it seamless to switch between providers without changing your application code."
icon: "layer-group"
---

## One Format, All Providers

The beauty of Bifrost lies in its unified interface: regardless of whether you're using OpenAI, Anthropic, AWS Bedrock, Google Vertex, or any other supported provider, you always get the same response format. This means your application logic never needs to change when switching providers.

Bifrost standardizes all provider responses to follow the **OpenAI-compatible structure**, so you can write your code once and use it with any provider.

## How It Works

When you make a request to any provider through Bifrost, the response always follows the same structure - the familiar OpenAI format that most developers already know. Behind the scenes, Bifrost handles all the complexity of translating between different provider formats.

<Tabs group="unified-interface">

<Tab title="Gateway">

```bash
# Same response format regardless of provider
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "openai/gpt-4o-mini",
    "messages": [{"role": "user", "content": "Hello!"}]
  }'

# Returns OpenAI-compatible format:
{
  "id": "chatcmpl-123",
  "object": "chat.completion", 
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "Hello! How can I help you?"
      },
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 10,
    "completion_tokens": 9,
    "total_tokens": 19
  }
}
```

</Tab>

<Tab title="Go SDK">

```go
// Same response structure regardless of provider
type BifrostChatResponse struct {
	ID                string                     `json:"id"`
	Choices           []BifrostResponseChoice    `json:"choices"`
	Created           int                        `json:"created"`
	Model             string                     `json:"model"`
	Object            string                     `json:"object"`
	ServiceTier       string                     `json:"service_tier"`
	SystemFingerprint string                     `json:"system_fingerprint"`
	Usage             *BifrostLLMUsage           `json:"usage"`
	ExtraFields       BifrostResponseExtraFields `json:"extra_fields"`
}

// Works with any provider
response, err := client.ChatCompletionRequest(ctx, &schemas.BifrostChatRequest{
    Provider: schemas.OpenAI,    // or Anthropic, Bedrock, etc.
    Model:    "gpt-4o-mini",     // or "claude-3-sonnet", etc.
    Input:    messages,
})
// Response structure is always the same!
```

</Tab>


</Tabs>

## Provider Support Matrix

The following table summarizes which operations are supported by each provider via Bifrostâ€™s unified interface.

| Provider | Models | Text | Text (stream) | Chat | Chat (stream) | Responses | Responses (stream) | Embeddings | TTS | TTS (stream) | STT | STT (stream) |
|----------|--------|------|----------------|------|---------------|-----------|--------------------|------------|-----|-------------|-----|--------------|
| Anthropic (`anthropic/<model>`) | âœ… | âœ… | âŒ | âœ… | âœ… | âœ… | âœ… | âŒ | âŒ | âŒ | âŒ | âŒ |
| Azure (`azure/<model>`) | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | âŒ | âŒ | âŒ | âŒ |
| Bedrock (`bedrock/<model>`) | âœ… | âœ… | âŒ | âœ… | âœ… | âœ… | âœ… | âœ… | âŒ | âŒ | âŒ | âŒ |
| Cerebras (`cerebras/<model>`) | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | âŒ | âŒ | âŒ | âŒ | âŒ |
| Cohere (`cohere/<model>`) | âœ… | âŒ | âŒ | âœ… | âœ… | âœ… | âœ… | âœ… | âŒ | âŒ | âŒ | âŒ |
| Elevenlabs (`elevenlabs/<model>`) | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ | âœ… | âœ… | âœ… | âŒ |
| Gemini (`gemini/<model>`) | âœ… | âŒ | âŒ | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… |
| Groq (`groq/<model>`) | âœ… | ğŸŸ¡ | ğŸŸ¡ | âœ… | âœ… | âœ… | âœ… | âŒ | âŒ | âŒ | âŒ | âŒ |
| Mistral (`mistral/<model>`) | âœ… | âŒ | âŒ | âœ… | âœ… | âœ… | âœ… | âœ… | âŒ | âŒ | âŒ | âŒ |
| Ollama (`ollama/<model>`) | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | âŒ | âŒ | âŒ | âŒ |
| OpenAI (`openai/<model>`) | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… |
| OpenRouter (`openrouter/<model>`) | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | âŒ | âŒ | âŒ | âŒ | âŒ |
| Parasail (`parasail/<model>`) | âœ… | âŒ | âŒ | âœ… | âœ… | âœ… | âœ… | âŒ | âŒ | âŒ | âŒ | âŒ |
| Perplexity (`perplexity/<model>`) | âœ… | âŒ | âŒ | âœ… | âœ… | âœ… | âœ… | âŒ | âŒ | âŒ | âŒ | âŒ |
| SGL (`sgl/<model>`) | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | âŒ | âŒ | âŒ | âŒ |
| Vertex AI (`vertex/<model>`) | âœ… | âŒ | âŒ | âœ… | âœ… | âœ… | âœ… | âœ… | âŒ | âŒ | âŒ | âŒ |

- ğŸŸ¡ Not supported by the downstream provider, but internally implemented by Bifrost as a fallback.
- âŒ Not supported by the downstream provider, hence not supported by Bifrost.
- âœ… Fully supported by the downstream provider, or internally implemented by Bifrost.


<Note>
Some operations are not supported by the downstream provider, and their internal implementation in Bifrost is optional. ğŸŸ¡
Like Text completions are not supported by Groq, but Bifrost can emulate them internally using the Chat Completions API. This feature is disabled by default, but it can be enabled by setting the `enable_litellm_fallbacks` flag to `true` in the client configuration.
We do not promote using such fallbacks, since text completions and chat completions are fundamentally different. However, this option is available to help users migrating from LiteLLM (which does support these fallbacks).
</Note>


Notes:
- â€œModelsâ€ refers to the list models operation (`/v1/models`).
- â€œTextâ€ refers to the classic text completion interface (`/v1/completions`).
- â€œResponsesâ€ refers to the OpenAI-style Responses API (`/v1/responses`). Non-OpenAI providers map this to their native chat API under the hood.
- TTS corresponds to `/v1/audio/speech` and STT to `/v1/audio/transcriptions`.

## The Power of Consistency

This unified approach means you can:

- **Switch providers instantly** without changing application logic
- **Mix and match providers** using fallbacks and load balancing
- **Future-proof your code** as new providers get added
- **Use familiar OpenAI patterns** regardless of the underlying provider

Whether you're calling OpenAI's GPT-4, Anthropic's Claude, or AWS Bedrock's models, your application sees the exact same response structure. This consistency is what makes Bifrost's advanced features like automatic fallbacks and multi-provider load balancing possible.

## Provider Transparency

While the response format stays consistent, Bifrost doesn't hide which provider actually handled your request. Provider information is always available in the `extra_fields` section, along with any provider-specific metadata you might need for debugging or analytics.

This gives you the best of both worlds: consistent application logic with full transparency into the underlying provider behavior.

**Learn more about configuring provider transparency:**
- **[Go SDK Provider Configuration](../quickstart/go-sdk/provider-configuration)** - Configure `SendBackRawResponse` and other provider settings
- **[Gateway Provider Configuration](../quickstart/gateway/provider-configuration)** - Configure `send_back_raw_response` via API, UI, or config file
