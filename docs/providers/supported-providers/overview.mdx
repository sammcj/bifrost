---
title: "Overview"
description: "Bifrost supports multiple AI providers with consistent OpenAI-compatible response formats, enabling seamless provider switching without code changes."
icon: "circle-info"
---

## Overview

Bifrost supports a wide range of AI providers, all accessible through a consistent OpenAI-compatible interface. This standardization allows you to switch between providers without modifying your application code, as all responses follow the same structure regardless of the underlying provider.
 
Bifrost can also act as a provider-compatible gateway (for example, <u>[Anthropic](../../integrations/anthropic-sdk/overview)</u>, <u>[Google Gemini](../../integrations/genai-sdk/overview)</u>, <u>Cohere</u>, <u>[Bedrock](../../integrations/bedrock-sdk/overview)</u>, and others), exposing provider-specific endpoints so you can use existing provider SDKs or integrations with no code changes, see [What is an integration?](../../integrations/what-is-an-integration) for details.


## Provider Support Matrix

The following table summarizes which operations are supported by each provider via Bifrostâ€™s unified interface.

| Provider | Models | Text | Text (stream) | Chat | Chat (stream) | Responses | Responses (stream) | Images | Images (stream) | Embeddings | TTS | TTS (stream) | STT | STT (stream) | Files | Batch | Count tokens |
|----------|--------|------|----------------|------|---------------|-----------|--------------------|--------|-----------------|------------|-----|-------------|-----|--------------|-------|-------|--------------|
| Anthropic (`anthropic/<model>`) | âœ… | âœ… | âŒ | âœ… | âœ… | âœ… | âœ… | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ | âœ… | âœ… | âœ… |
| Azure (`azure/<model>`) | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | âŒ | âœ… | âœ… | âŒ |
| Bedrock (`bedrock/<model>`) | âœ… | âœ… | âŒ | âœ… | âœ… | âœ… | âœ… | âœ… | âŒ | âœ… | âŒ | âŒ | âŒ | âŒ | âœ… | âœ… | âŒ |
| Cerebras (`cerebras/<model>`) | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ |
| Cohere (`cohere/<model>`) | âœ… | âŒ | âŒ | âœ… | âœ… | âœ… | âœ… | âŒ | âŒ | âœ… | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ | âœ… |
| Elevenlabs (`elevenlabs/<model>`) | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ | âœ… | âœ… | âœ… | âŒ | âŒ | âŒ | âŒ |
| Gemini (`gemini/<model>`) | âœ… | âŒ | âŒ | âœ… | âœ… | âœ… | âœ… | âœ… | âŒ | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… |
| Groq (`groq/<model>`) | âœ… | ğŸŸ¡ | ğŸŸ¡ | âœ… | âœ… | âœ… | âœ… | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ |
| Hugging Face (`huggingface/<model>`) | âœ… | âŒ | âŒ | âœ… | âœ… | âœ… | âœ… | âœ…| âœ… | âœ… | âœ… | âŒ | âœ… | âŒ | âŒ | âŒ | âŒ |
| Mistral (`mistral/<model>`) | âœ… | âŒ | âŒ | âœ… | âœ… | âœ… | âœ… | âŒ | âŒ | âœ… | âŒ | âŒ | âœ… | âœ… | âŒ | âŒ | âŒ |
| Nebius (`nebius/<model>`) | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | âŒ | âœ… | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ |
| Ollama (`ollama/<model>`) | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | âŒ | âŒ | âœ… | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ |
| OpenAI (`openai/<model>`) | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… |
| OpenRouter (`openrouter/<model>`) | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ |
| Parasail (`parasail/<model>`) | âœ… | âŒ | âŒ | âœ… | âœ… | âœ… | âœ… | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ |
| Perplexity (`perplexity/<model>`) | âœ… | âŒ | âŒ | âœ… | âœ… | âœ… | âœ… | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ |
| SGL (`sgl/<model>`) | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | âŒ | âŒ | âœ… | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ |
| Vertex AI (`vertex/<model>`) | âœ… | âŒ | âŒ | âœ… | âœ… | âœ… | âœ… | âœ… | âŒ | âœ… | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ | âœ… |
| xAI (`xai/<model>`) | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ |


- ğŸŸ¡ Not supported by the downstream provider, but internally implemented by Bifrost as a fallback.
- âŒ Not supported by the downstream provider, hence not supported by Bifrost.
- âœ… Fully supported by the downstream provider, or internally implemented by Bifrost.


<Note>
Some operations are not supported by the downstream provider, and their internal implementation in Bifrost is optional. ğŸŸ¡
Like Text completions are not supported by Groq, but Bifrost can emulate them internally using the Chat Completions API. This feature is disabled by default, but it can be enabled by setting the `enable_litellm_fallbacks` flag to `true` in the client configuration.
We do not promote using such fallbacks, since text completions and chat completions are fundamentally different. However, this option is available to help users migrating from LiteLLM (which does support these fallbacks).
</Note>


Notes:
- "Models" refers to the list models operation (`/v1/models`).
- "Text" refers to the classic text completion interface (`/v1/completions`).
- "Responses" refers to the OpenAI-style Responses API (`/v1/responses`). Non-OpenAI providers map this to their native chat API under the hood.
- "Images" refers to the Image Generation API (`/v1/images/generations`).
- TTS corresponds to `/v1/audio/speech` and STT to `/v1/audio/transcriptions`.
- "Files" refers to the Files API operations (`/v1/files`) for uploading, listing, retrieving, and deleting files.
- "Batch" refers to the Batch API operations (`/v1/batches`) for creating, listing, retrieving, canceling, and getting results of batch jobs.


## Response Format

All providers return responses in the OpenAI-compatible format. Bifrost handles the translation between different provider-specific formats automatically.

<Tabs>
<Tab title="Gateway">

```bash
# Same response format regardless of provider
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "openai/gpt-4o-mini",
    "messages": [{"role": "user", "content": "Hello!"}]
  }'

# Returns OpenAI-compatible format:
{
  "id": "chatcmpl-123",
  "object": "chat.completion",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "Hello! How can I help you?"
      },
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 10,
    "completion_tokens": 9,
    "total_tokens": 19
  }
}
```

</Tab>

<Tab title="Go SDK">

```go
// Same response structure regardless of provider
type BifrostChatResponse struct {
	ID                string                     `json:"id"`
	Choices           []BifrostResponseChoice    `json:"choices"`
	Created           int                        `json:"created"`
	Model             string                     `json:"model"`
	Object            string                     `json:"object"`
	ServiceTier       string                     `json:"service_tier"`
	SystemFingerprint string                     `json:"system_fingerprint"`
	Usage             *BifrostLLMUsage           `json:"usage"`
	ExtraFields       BifrostResponseExtraFields `json:"extra_fields"`
}

// Works with any provider
response, err := client.ChatCompletionRequest(ctx, &schemas.BifrostChatRequest{
    Provider: schemas.OpenAI,    // or Anthropic, Bedrock, etc.
    Model:    "gpt-4o-mini",     // or "claude-3-sonnet", etc.
    Input:    messages,
})
// Response structure is always the same!
```

</Tab>
</Tabs>


## Custom Providers

In addition to the built-in providers, Bifrost supports custom provider configurations. Custom providers allow you to create multiple instances of the same base provider with different configurations, request type restrictions, and access patterns. This is useful for environment-specific configurations, role-based access control, and feature testing.

**Learn more:** [Custom Providers](../custom-providers)

## Benefits

The consistent interface across providers enables:

- **Provider switching** without code modifications
- **Fallback configurations** for improved reliability
- **Load balancing** across multiple providers
- **OpenAI-compatible patterns** for all providers

## Provider Metadata

Provider information is included in the `extra_fields` section of each response, providing transparency into which provider handled the request and any provider-specific metadata.

### Raw Request/Response Access

Bifrost can optionally return the raw request that was sent to the provider and the raw response received back. This is useful for debugging, auditing, and understanding how Bifrost transforms requests between different provider formats.

**What's included:**
- **`raw_request`** - The exact request body (JSON/data structure) that was sent to the provider's API endpoint
- **`raw_response`** - The exact response body received from the provider (before Bifrost's normalization)
- **Provider transformation details** - Shows exactly how Bifrost converted your input to provider-specific format

**Example**: When you send a Chat Completions request with `max_completion_tokens` to Anthropic, Bifrost converts it to `max_tokens` in the raw request. Enabling raw request/response reveals this transformation.

```json
{
  "extra_fields": {
    "provider": "anthropic",
    "raw_request": {
      "model": "claude-3-5-sonnet",
      "max_tokens": 4096,
      "messages": [...]
    },
    "raw_response": {
      "id": "msg_...",
      "type": "message",
      "content": [...],
      "usage": {
        "input_tokens": 123,
        "output_tokens": 456
      }
    }
  }
}
```

**Use cases:**
- **Debugging** - Verify how your request was transformed for the specific provider
- **Auditing** - Track exactly what was sent to external APIs
- **Cost analysis** - See actual token counts before Bifrost's normalization
- **Integration testing** - Validate provider-specific transformations

**Configuration options:**
- **[Go SDK Provider Configuration](../../quickstart/go-sdk/provider-configuration)** - Configure `SendBackRawResponse` and other provider settings
- **[Gateway Provider Configuration](../../quickstart/gateway/provider-configuration)** - Configure `send_back_raw_response` via API, UI, or config file

<Note>
Enabling raw request/response may increase response payload size and has minimal performance impact. Use it selectively in debugging/testing environments or when you need audit trails.
</Note>
