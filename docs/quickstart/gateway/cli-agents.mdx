---
title: "Tools, Editors & CLI Agents"
description: "Use Bifrost with tools like LibreChat, Claude Code, Codex CLI, Gemini CLI and Qwen Code by just changing the base URL and unlock advanced features."
icon: "robot"
---

## Overview

Bifrost provides **100% compatible endpoints** for OpenAI, Anthropic, and Gemini APIs, making it seamless to integrate with any agent that uses these providers. By simply pointing your agent's base URL to Bifrost, you unlock powerful features like:

- **Universal Model Access**: Use **any provider/model** configured in Bifrost with any agent (e.g., use GPT-5 with Claude Code, or Claude Sonnet 4.5 with Codex CLI)
- **MCP Tools Integration**: All Model Context Protocol tools configured in Bifrost become available to your agents
- **Built-in Observability**: Monitor all agent interactions in real-time through Bifrost's logging dashboard
- **Load Balancing**: Automatically distribute requests across multiple providers and regions
- **Advanced Features**: Governance, caching, failover, and more - all transparent to your agent

## Example Integrations

### [LibreChat](https://github.com/danny-avila/LibreChat)

It is a modern, open-source chat client that supports multiple providers.

**Setup:**

1. **Install LibreChat:** There are multiple ways of local setup, please follow the [LibreChat documentation](https://www.librechat.ai/docs/local) for more details.

2. **Add Bifrost as a custom provider**: Now that you have LibreChat installed, you can add Bifrost as a custom provider.

   Add the following to your `librechat.yaml` file:
   ```yaml
   custom:
    - name: "Bifrost"
      apiKey: "dummy" # Add the authentication key if login is enabled, otherwise add a placeholder
      baseURL: "http://host.docker.internal:8080/v1" # Or localhost:8080 if running locally, or {your-bifrost-container}:8080 if running in the same docker network
      models:
        default: ["openai/gpt-4o"] # Replace with the model you want to use
        fetch: true
      titleConvo: true
      titleModel: "openai/gpt-4o" # Replace with the model you want to use for chat title generation
      summarize: false # Set to true if you want to enable chat summary generation
      summaryModel: "openai/gpt-4o" # Replace with the model you want to use for chat summary generation
      forcePrompt: false # Set to true if you want to enable force prompt generation
      modelDisplayLabel: "Bifrost" 
      iconURL: https://getbifrost.ai/bifrost-logo.png
   ```

   <Note>
   If you're running LibreChat in a docker container, LibreChat does not automatically use the `librechat.yaml` file, please check the Step 1 of the [LibreChat documentation](https://www.librechat.ai/docs/quick_start/custom_endpoints#step-1-create-or-edit-a-docker-override-file) for more details.
   </Note>

3. **Run LibreChat**

   Now you can start using Bifrost as a provider in LibreChat, with all the features of Bifrost.

### [Claude Code](https://www.claude.com/product/claude-code)

It brings Sonnet 4.5 directly to your terminal with powerful coding capabilities.

**Setup:**

1. **Install Claude Code**
   ```bash
   npm install -g @anthropic-ai/claude-code
   ```

2. **Configure Environment Variables**
   ```bash
   export ANTHROPIC_API_KEY=dummy-key  # Handled by Bifrost (only set when using virtual keys)
   export ANTHROPIC_BASE_URL=http://localhost:8080/anthropic
   ```

3. **Run Claude Code**
   ```bash
   claude
   ```

Now all Claude Code traffic flows through Bifrost, giving you access to any provider/model configured in your Bifrost setup, plus MCP tools and observability.

<Note>
This setup automatically detects if you're using Anthropic MAX account instead of a regular API key authentication :)
</Note>

### [Codex CLI](https://developers.openai.com/codex/cli/)

It provides powerful code generation and completion capabilities.

**Setup:**

1. **Install Codex CLI**
   ```bash
   npm install -g @openai/codex
   ```

2. **Configure Environment Variables**
   ```bash
   export OPENAI_API_KEY=dummy-key  # Handled by Bifrost (only set when using virtual keys)
   export OPENAI_BASE_URL=http://localhost:8080/openai
   ```

3. **Run Codex**
   ```bash
   codex
   ```

### [Gemini CLI](https://github.com/google-gemini/gemini-cli)

It is Google's powerful coding assistant with advanced reasoning capabilities.

**Setup:**

1. **Install Gemini CLI**
   ```bash
   npm install -g @google/gemini-cli
   ```

2. **Configure Environment Variables**
   ```bash
   export GEMINI_API_KEY=dummy-key  # Handled by Bifrost (only set when using virtual keys)
   export GOOGLE_GEMINI_BASE_URL=http://localhost:8080/genai
   ```

3. **Run Gemini CLI**
   ```bash
   gemini
   ```

4. Select **Use Gemini API Key** in the CLI prompt for authentication.

![Gemini CLI authentication](../../media/gemini-cli.png)

### [Qwen Code](https://github.com/QwenLM/qwen-code)

It is Alibaba's powerful coding assistant with advanced reasoning capabilities.

**Setup:**

1. **Install Qwen Code**
   ```bash
    npm install -g @qwen-code/qwen-code
   ```

2. **Configure Base URL**
   ```bash
   export OPENAI_BASE_URL=http://localhost:8080/openai
   ```

3. **Run Qwen Code**
   ```bash
   qwen
   ```

### [Opencode](https://github.com/sst/opencode)

![opencode with Bifrost](../../media/opencode-with-bifrost.png)


**Setup**

1. **Configure Bifrost**

```json
{
  "$schema": "https://opencode.ai/config.json",
  // Theme configuration
  "theme": "opencode",
  "autoupdate": true,
  "provider": {
    "openai": {
      "name": "Bifrost",
      "options": {        
        "baseURL": "http://localhost:8080/openai",
        "apiKey": "{{virtual-key-if-enabled}}"
      },
      "models": {
        "openai/gpt-5": {
          "options": {
            "reasoningEffort": "high",
            "textVerbosity": "low",
            "reasoningSummary": "auto",
            "include": [
              "reasoning.encrypted_content"
            ],
          },
        },
        "anthropic/claude-sonnet-4-5-20250929": {
          "options": {
            "thinking": {
              "type": "enabled",
              "budgetTokens": 16000,
            },
          },
        },
      },
    }
  }
}
```

2. Select Bifrost models using <key>ctrl</key>+<key>p</key>

![Opencode model selection](../../media/opencode-model-selection.png)

## Editors

### [Zed editor](https://zed.dev/)

![Zed editor](../../media/zed-editor-integration.png)

1. **Configure Bifrost provider.**

```json {4}
   "language_models": {
        "openai_compatible": {
            "Bifrost": {
                "api_url": "{{bifrost-base-url}}/openai",
                "available_models": [
                    {
                        "name": "anthropic/claude-sonnet-4.5",
                        "max_tokens": 200000,
                        "max_output_tokens": 4096,
                        "capabilities": {
                            "tools": true,
                            "images": true,
                            "parallel_tool_calls": true,
                            "prompt_cache_key": false
                        }
                    },
                    {
                        "name": "openai/gpt-4o",
                        "max_tokens": 128000,
                        "max_output_tokens": 4096,
                        "capabilities": {
                            "tools": true,
                            "images": true,
                            "parallel_tool_calls": true,
                            "prompt_cache_key": false
                        }
                    },
                    {
                        "name": "openai/gpt-5",
                        "max_tokens": 256000,
                        "max_output_tokens": 4096,
                        "capabilities": {
                            "tools": true,
                            "images": true,
                            "parallel_tool_calls": true,
                            "prompt_cache_key": false
                        }
                    }
                ]
            }
        }
    }
```

2. **Reload workspace** to make sure Zed editor recognizes and reloads the provider list.

## Configuration

Agent integrations work with your existing Bifrost configuration. Ensure you have:

- **Providers configured**: See [Provider Configuration](./provider-configuration) for setup details
- **Optional: MCP tools**: See [MCP Integration](../../features/mcp) to enhance agent capabilities

## Monitoring Agent Traffic

All agent interactions are automatically logged and can be monitored at `http://localhost:8080/logs`. You can filter by provider, model, or search through conversation content to track your agents' performance.

![Agent Monitoring](../../media/ui-live-log-stream.gif)
For complete monitoring capabilities, see [Built-in Observability](../../features/observability/default).

## MCP Tools Integration

Bifrost automatically sends all configured MCP tools to your agents. This means your agents can access filesystem operations, database queries, web search, and more without any additional configuration.

<Note>
**Important: MCP Tool Execution Behavior**

When using Bifrost as a gateway, MCP tool calls require manual approval and execution for security reasons. Bifrost returns the tool call information but doesn't automatically execute it. You need to handle the approval and execution logic by calling the `v1/mcp/tool/execute` endpoint.

**Gateway-on-Gateway Limitations**: If your agent/editor (like Zed) has its own gateway that routes through Bifrost, the agent's gateway may not handle MCP tool approvals that come from Bifrost. In such cases, we recommend configuring MCP tools directly in your agent/editor instead of relying on Bifrost's MCP integration.

We intentionally avoid supporting "gateway-on-gateway" MCP setups because handling tool approvals across multiple gateways introduces unnecessary complexity and falls outside the scope of what an LLM gateway should manage. While we're working on an agentic mode that will allow Bifrost to automatically execute certain tool calls, the current design prioritizes security and clear responsibility boundaries.
</Note>

For setup and available tools, see [MCP Integration](../../features/mcp).

## Next Steps

- **[Provider Configuration](./provider-configuration)** - Configure AI providers for your agents
- **[Governance](../../features/governance)** - Set usage limits and policies for your agents
- **[Integrations](../../integrations/what-is-an-integration)** - Understand how Bifrost works with existing AI provider SDKs