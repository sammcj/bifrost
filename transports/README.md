# Bifrost Transports

This package contains clients for various transports that can be used to spin up your Bifrost client with just a single line of code.

üìñ **Comprehensive HTTP API documentation is available in** _[`docs/http-transport-api.md`](../docs/http-transport-api.md)_.

## üìë Table of Contents

- [Bifrost Transports](#bifrost-transports)
  - [üìë Table of Contents](#-table-of-contents)
  - [üöÄ Setting Up Transports](#-setting-up-transports)
    - [Prerequisites](#prerequisites)
    - [Configuration](#configuration)
    - [Docker Setup](#docker-setup)
    - [Go Setup](#go-setup)
  - [üß∞ Usage](#-usage)
    - [Text Completions](#text-completions)
    - [Chat Completions](#chat-completions)
  - [üîß Advanced Features](#-advanced-features)
    - [Prometheus Support](#prometheus-support)
    - [Plugin Support](#plugin-support)
    - [Fallbacks](#fallbacks)

---

## üöÄ Setting Up Transports

### Prerequisites

- Go 1.23 or higher (if not using Docker)
- Access to at least one AI model provider (OpenAI, Anthropic, etc.)
- API keys for the providers you wish to use

### Configuration

Bifrost uses a combination of a JSON configuration file and environment variables:

1. **JSON Configuration File**: Bifrost requires a configuration file to set up the gateway. This includes all your provider-level settings, keys, and meta configs for each of your providers.
2. **Environment Variables**: If you don't want to include your keys in your config file, you can add a prefix of `env.` followed by its key in your environment.

```json
{
  "openai": {
    "keys": [
      {
        "value": "env.OPENAI_API_KEY",
        "models": ["gpt-4o-mini"],
        "weight": 1.0
      }
    ]
  }
}
```

In this example config file, `OPENAI_API_KEY` refers to a key set in your environment. At runtime, its value will be used to replace the placeholder.

The same setup applies to keys in meta configs of all providers:

```json
{
  "meta_config": {
    "secret_access_key": "env.AWS_SECRET_ACCESS_KEY",
    "region": "env.AWS_REGION"
  }
}
```

In this example, `AWS_SECRET_ACCESS_KEY` and `AWS_REGION` refer to keys in the environment.

**Please refer to `config.example.json` for examples.**

### Docker Setup

1. Pull the Docker image:

   ```bash
   docker pull maximhq/bifrost
   ```

2. Run the Docker container:

   ```bash
   docker run -p 8080:8080 \
     -v $(pwd)/config.json:/app/config/config.json \
     -e OPENAI_API_KEY \
     -e ANTHROPIC_API_KEY \
     maximhq/bifrost
   ```

Note: In the command above, `OPENAI_API_KEY` and `ANTHROPIC_API_KEY` are just example environment variables.
Ensure you mount your config file and use the `-e` flag to pass all environment variables referenced in your `config.json` that are prefixed with `env.` to the container. This ensures Docker sets them correctly inside the container.

Example usage: Suppose your config.json only contains one environment variable placeholder, `env.COHERE_API_KEY`. Here's how you would run it:

```bash
export COHERE_API_KEY=your_cohere_api_key

docker run -p 8080:8080 \
  -v $(pwd)/config.example.json:/app/config/config.json \
  -e COHERE_API_KEY \
  maximhq/bifrost
```

You can also set runtime environment variables for configuration:

- `APP_PORT`: Server port (default: 8080)
- `APP_POOL_SIZE`: Connection pool size (default: 300)
- `APP_DROP_EXCESS_REQUESTS`: Drop excess requests when buffer is full (default: false)
- `APP_PLUGINS`: Comma-separated list of plugins

Read more about these [configurations](https://github.com/maximhq/bifrost/tree/main?tab=README-ov-file#additional-configurations).

---

### Go Setup

If you wish to run Bifrost in your Go environment, follow these steps:

1. Install your binary:

   ```bash
   go install github.com/maximhq/bifrost/transports/bifrost-http@latest
   ```

2. Run your binary (make sure Go is set in your PATH):

```bash
bifrost-http -config config.json -port 8080 -pool-size 300
```

You can also add a flag for `-drop-excess-requests=false` in your command to drop excess requests when the buffer is full. Read more about `DROP_EXCESS_REQUESTS` and `POOL_SIZE` in [additional configurations](https://github.com/maximhq/bifrost/tree/main?tab=README-ov-file#additional-configurations).

## üß∞ Usage

Ensure that:

- Bifrost's HTTP server is running
- The providers/models you use are configured in your JSON config file

### Text Completions

```bash
# Make sure to setup anthropic and claude-2.1 in your config.json
curl -X POST http://localhost:8080/v1/text/completions \
  -H "Content-Type: application/json" \
  -d '{
    "provider": "anthropic",
    "model": "claude-2.1",
    "text": "Once upon a time in the land of AI,",
    "params": {
      "temperature": 0.7,
      "max_tokens": 100
    }
  }'
```

### Chat Completions

```bash
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "provider": "openai",
    "model": "gpt-4o-mini",
    "messages": [
      {"role": "system", "content": "You are a helpful assistant."},
      {"role": "user", "content": "Tell me about Bifrost in Norse mythology."}
    ],
    "params": {
      "temperature": 0.8,
      "max_tokens": 500
    }
  }'
```

---

## üîß Advanced Features

### Prometheus Support

HTTP transport supports Prometheus out of the box. By default all the metrics are available at `/metrics` endpoint. It providers metrics for httpRequestsTotal, httpRequestDuration, httpRequestSizeBytes, httpResponseSizeBytes, bifrostUpstreamRequestsTotal, and bifrostUpstreamLatencySeconds. To add custom labels to these metrics using can pass a flag of `-prometheus-labels` while running the http transport.

e.g., `-prometheus-labels team-id,task-id,location`

Values for labels are then picked up from the HTTP request headers with the prefix `x-bf-prom-`.

### Plugin Support

You can explore the [available plugins](https://github.com/maximhq/bifrost/tree/main/plugins). To attach these plugins to your HTTP transport, pass the `-plugins` flag.

e.g., `-plugins maxim`

Note: Please check plugin specific documentations (github.com/maximhq/bifrost/tree/main/plugins/{plugin_name}) for more nuanced control and any additional setup.

### Fallbacks

Configure fallback options in your requests:

```json
{
  "provider": "openai",
  "model": "gpt-4",
  "messages": [...],
  "fallbacks": [
    {
      "provider": "anthropic",
      "model": "claude-3-opus-20240229"
    },
    {
      "provider": "bedrock",
      "model": "anthropic.claude-3-sonnet-20240229-v1:0"
    }
  ]
}
```

Read more about fallbacks and other [additional configurations](https://github.com/maximhq/bifrost/tree/main?tab=README-ov-file#additional-configurations).

Built with ‚ù§Ô∏è by [Maxim](https://github.com/maximhq)
